{
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Predictive Analysis of Survival Rate on Titanic\n===============================================\n\nThis is a predictive machine learning project using ```R``` based on Kaggle competition: [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic). \n\n----------\n**Content**\n\n1. Introduction\n\n      1.1 Objective\n\n      1.2 Data Understanding\n\n2. Data Preparation and Exploratory Analysis\n\n      2.1 Data Cleaning\n\n      2.2 Exploratory Analysis and Data Processing\n\n3. Modeling\n\n      3.1 Feature Engineering\n\n      3.2 Model Training\n\n      3.3 Model  Evaluation\n\n4. Prediction\n\n----------",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 1. Introduction\n\n### 1.1 Objective\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we are going to complete the analysis of what sorts of people were likely to survive.\n\n### 1.2 Data Understanding\n\nTo download data, click [here][1]\n\n\n  [1]: https://www.kaggle.com/c/titanic/data\n\nThe data has been split into two groups:\n\n* training set (train.csv)\n* test set (test.csv)\n\nThe training set is used to build machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n**Data Dictionary**\n\n| Variable  |  Definition | Key  |   \n|-----------|-------------|---------|\n| survival  |  Survival   |  0 = No, 1 = Yes |  \n|  pclass\t |  Ticket class |  1 = 1st, 2 = 2nd, 3 = 3rd |  \n|  sex |  Sex |   |   \n|  Age |   Age in years |   |  \n|  sibsp |  # of siblings / spouses aboard the Titanic |   |   \n| parch  | # of parents / children aboard the Titanic  |   |   \n|  ticket | Ticket number  |   |  \n|  fare | Passenger fare  |   |   \n| cabin  |  Cabin number  |   |   \n| embarked  | Port of Embarkation  |C = Cherbourg, Q = Queenstown, S = Southampton   |   \n\n**Variable Notes**\n\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancés were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\n\nFirst, let's load the data and take a look at it.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "library('dplyr') # data manipulation\nlibrary('ggplot2') # Data Visualization\nlibrary('ggthemes') # Data Visualization\n\noptions(warn = -1)\n# load train.csv\ntrain <- read.csv('../input/train.csv', stringsAsFactors = F)\n# load test.csv\ntest  <- read.csv('../input/test.csv', stringsAsFactors = F)\n# combine them as a whole\ntest$Survived <- NA\nfull <- rbind(train,test)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# show first several rows of the data\nhead(full)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n# check the data\nstr(full)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We've got a sense of our variables, their class type, and the first few observations of each. We know we're working with 1309 observations of 12 variables. In which 891 observations are from train data set, and 418 observations are from test data set. When separate the variables by type, we have ordinal variable **PassengerId**, lable variable **Name** and **Ticket**, numeric variables such as **Age**, **SibSp**, **Parch**, **Fare**, and categorical variables like **Survived** ,**Pclass**, **Sex** ,**Cabin**, and  **Embarked**. \n\n## 2. Data Preparation and Exploratory Analysis\n\n### 2.1 Data Cleaning\n\nFrom the data set, we notice that there are missing values in **Age**, **Cabin** ,**Fare** and **Embarked** column. We are going to replace missing values in Age with a random sample from existing ages. For Cabin, since cabin number makes little sense to the result, we are going to create a new Cabin column to indicate how many cabins the passenger has. \n  ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Process Age Column\n\n    # create a new data set age\nage <- full$Age\nn = length(age)\n    # replace missing value with a random sample from raw data\nset.seed(123)\nfor(i in 1:n){\n  if(is.na(age[i])){\n    age[i] = sample(na.omit(full$Age),1)\n  }\n}\n    # check effect\npar(mfrow=c(1,2))\nhist(full$Age, freq=F, main='Before Replacement', \n  col='lightblue', ylim=c(0,0.04),xlab = \"age\")\nhist(age, freq=F, main='After Replacement', \n  col='darkblue', ylim=c(0,0.04))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can see from the histograms above that there is not much significant change of age distribution, which means the replacement is appropriate. Next we are going to process Cabin Column. We are going to create a new Cabin column to indicate how many cabins the passenger has.  \n  ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Process Cabin Column to show number of cabins passenger has\ncabin <- full$Cabin\nn = length(cabin)\nfor(i in 1:n){\n  if(nchar(cabin[i]) == 0){\n    cabin[i] = 0\n  } else{\n    s = strsplit(cabin[i],\" \")\n    cabin[i] = length(s[[1]])\n  }\n} \ntable(cabin)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# for Fare column, we repace the missing value with median fare of according Pclass and Embarked\nfull$Fare[1044] <- median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm = TRUE)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# process embarked column\nembarked <- full$Embarked\nn = length(embarked)\nfor(i in 1:n){\n  if(embarked[i] != \"S\" && embarked[i] != \"C\" && embarked[i] != \"Q\"){\n    embarked[i] = \"S\"\n  }\n}\ntable(embarked)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 2.1 Exploratory Analysis and Data Processing\n\nAs our objective is to figure out  what features would influence the survival, we are going to go deep into the data to explore the relationship between each attribute and survival.\n\n**Age** v.s **Survival**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# number of survivals and nonsurvivals across different age\nd <- data.frame(Age = age[1:891], Survived = train$Survived)\nggplot(d, aes(Age,fill = factor(Survived))) +\n    geom_histogram()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "From the histogram, it seems that kids with very young age have a respectively higher survival rate, and elder people have a respectively lower survival rate. To verify it, I create a bar chart to show the relationship between survival rate and age intervals.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# create bar chart to show relationship between survival rate and age intervals\ncuts <- cut(d$Age,hist(d$Age,10,plot = F)$breaks)\nrate <- tapply(d$Survived,cuts,mean)\nd2 <- data.frame(age = names(rate),rate)\nbarplot(d2$rate, xlab = \"age\",ylab = \"survival rate\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can see clearly from the bar chart above that **survival rate decreases as age increases**. Kids below 10 years old have a higher survival rate above 0.5, people who's age is between 10 to 60 have a relatively constant survival rate around 0.4, and elder people above 60 years old has a lower survival rate around 0.2.\n\n\n----------\n\n\n**Sex** v.s **Survival**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# create histgram to show effect of Sex on survival\nggplot(train, aes(Sex,fill = factor(Survived))) +\n    geom_histogram(stat = \"count\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can see from the histogram above that **female's survival rate is greater than male's**.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# calculate survival rate\ntapply(train$Survived,train$Sex,mean)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The survival rate of female is 0.74, while the survival rate of male is 0.19.\n\n\n----------\n\n**Name** v.s. **Survival**\n\nWe also notice that title of surname is a meaningful feature. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# extract title from Name \n# (here I process full data set but only plot title vs survival in train \n#    data set because there is no survival value for test data set)\nn = length(full$Survived)\ntitle = rep(NA,n)\nfor (i in 1:n){\n  lastname = strsplit(full$Name[i],\", \")[[1]][2]\n  title[i] = strsplit(lastname,\". \")[[1]][1]\n}\n\n# make a histogram of title v.s survival\nd <- data.frame(title = title[1:891],Survived = train$Survived)\nggplot(d, aes(title,fill = factor(Survived))) +\n    geom_histogram(stat = \"count\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To see clearly the survival rate for each group, we also make a table.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# count of title\ntable(title)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# survival rate\ntapply(d$Survived,d$title,mean)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can see from the table, survival rates of females with Miss and Mrs title are close to the average survival rate for female group calculated in last section. Survival rates of males with Master are higher than the average male group. Titles like Col, Rev, Dr etc. also have influence on the survival.  \n\nSince number of  each rare title is much smaller than the majorities, we replace these rare titles to 'Rare'.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# replace rare titles to 'Rare'\ntitle[title != 'Mr' & title != 'Miss' & title != 'Mrs' & title != 'Master'] <- 'Rare'\ntable(title)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n----------\n\n\n**Pclass** v.s. **Survival**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# make a histogram\nggplot(train, aes(Pclass,fill = factor(Survived))) +\n    geom_histogram(stat = \"count\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# calculate survival rate\ntapply(train$Survived,train$Pclass,mean)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "From the histogram and table, we notice that **Pclass = 1 group has the highest survival rate, then is Pclass = 2 group, and Pclass = 3  group has the lowest survival rate within these three groups**.\n\n\n----------\n\n**Family Size** v.s. **Survival**\n\nWe first check SibSp and Parch column separately.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# histogram of Parch\nggplot(train, aes(Parch,fill = factor(Survived))) +\n    geom_histogram(stat = \"count\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# histogram of SibSp\nggplot(train, aes(SibSp,fill = factor(Survived))) +\n    geom_histogram(stat = \"count\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can see that they have similar trend, then we decide to combine them together to construct a column named family.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# combine SibSp and Parch \nfamily <- full$SibSp + full$Parch\nd <- data.frame(family = family[1:891],Survived = train$Survived)\nggplot(d, aes(family,fill = factor(Survived))) +\n    geom_histogram(stat = \"count\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tapply(d$Survived,d$family,mean)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can see that **the survival rate increases as the family size increases from 0 to 3. When family size becomes greater than 3, survival rate decrease dramatically.** \n\n\n----------\n\n**Cabin** v.s. **Survival**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# create histogram\nd <- data.frame(Cabin = cabin[1:891],Survived = train$Survived)\nggplot(d, aes(Cabin,fill = factor(Survived))) +\n    geom_histogram(stat = \"count\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# calculate survival rate\ntapply(d$Survived,d$Cabin,mean)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We notice that **passenger who has no cabin has a lower survival rate, and passenger who has one or more cabins has higher survival rate.**\n\n\n----------\n\n**Fare** v.s. **Survival**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# make a histogram\nggplot(train, aes(Fare,fill = factor(Survived))) +\n    geom_histogram()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# calculate\ncuts <- cut(train$Fare,hist(train$Fare,10,plot = F)$breaks)\nrate <- tapply(train$Survived,cuts,mean)\nd <- data.frame(fare = names(rate),rate)\nbarplot(d$rate, xlab = \"fare\",ylab = \"survival rate\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We notice that **Passengers who's fare is lower than 50 has a relatively lower survival rate. Passengers who's fare is extremely high (500-550) have very high survival rate.**\n\n\n----------\n\n**Embarked** v.s. **Survival**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# make histogram\nd <- data.frame(Embarked = embarked[1:891], Survived = train$Survived)\nggplot(d, aes(Embarked,fill = factor(Survived))) +\n    geom_histogram(stat = \"count\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# make table\ntapply(train$Survived,train$Embarked,mean)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We notice that Embarked C group has a relatively higher survival rate than other 2 groups.\n\n## 3. Modeling\n\n### 3.1 Feature Engineering\n\nIn this section, we are going to prepare features used for training and predicting. We first choose our features that have significant effect on survival according to the exploratory process above. Here we choose Survived column as response variable, age (after filling), title, Pclass, Sex, family size, Fare, cabin(cabin count), Embarked these 8 column as features. \n\n| Response Variable (Y)  |  Features (X)  |   \n|-----------|-------------|---------|-----------|-------------|---------|-----------|-------------|---------|\n| Survived  |  age ,fare, cabin , title  , family , Pclass,   Sex ,  Embarked  | ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# response variable\nf.survived = train$Survived",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# feature\n# 1. age\nf.age = age[1:891]    # for training\nt.age = age[892:1309]  # for testing",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# 2. fare\nf.fare = full$Fare[1:891]\nt.fare = full$Fare[892:1309]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# 3. cabin\nf.cabin = cabin[1:891]\nt.cabin = cabin[892:1309]\n\n# 4. title\nf.title = title[1:891]\nt.title = title[892:1309]\n\n# 5. family\nfamily <- full$SibSp + full$Parch\nf.family = family[1:891]\nt.family = family[892:1309]\n\n# 6. plcass\nf.pclass = train$Pclass\nt.pclass = test$Pclass\n\n# 7. sex\nf.sex = train$Sex\nt.sex = test$Sex\n\n# 8. embarked\nf.embarked = embarked[1:891]\nt.embarked = embarked[892:1309]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 3.2 Model Training\n\nWe tried to build basic learners using common machine learning model such as Logistic Regression, Decision Tree, Random Forest, SVM. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# construct training data frame\nnew_train = data.frame(survived = f.survived, age = f.age, fare = f.fare , sex = f.sex, \n       embarked = f.embarked ,family = f.family ,title = f.title ,cabin =  f.cabin, pclass= f.pclass)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# logistic regression\nfit_logit <- glm(factor(survived) ~ age + fare + sex + embarked + family \n                 + title + cabin + pclass,data = new_train,family = binomial)\n    # predicted result of regression\nans_logit = rep(NA,891)\nfor(i in 1:891){\n  ans_logit[i] = round(fit_logit$fitted.values[[i]],0)\n}\n    # check result\nmean(ans_logit == train$Survived)\ntable(ans_logit)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# random forest\nlibrary('randomForest')\n\nset.seed(123)\nfit_rf <- randomForest(factor(survived) ~ age + fare + sex + embarked + family \n                 + title + cabin + pclass,data = new_train)\n\n    # predicted result of regression\nrf.fitted = predict(fit_rf)\nans_rf = rep(NA,891)\nfor(i in 1:891){\n  ans_rf[i] = as.integer(rf.fitted[[i]]) - 1\n}\n    # check result\nmean(ans_rf == train$Survived)\ntable(ans_rf)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# decision tree\nlibrary(rpart)\n\nfit_dt <- rpart(factor(survived) ~ age + fare + sex + embarked + family \n                 + title + cabin + pclass,data = new_train)\n\n    # predicted result of regression\ndt.fitted = predict(fit_dt)\nans_dt = rep(NA,891)\nfor(i in 1:891){\n  if(dt.fitted[i,1] >= dt.fitted[i,2] ){\n    ans_dt[i] = 0\n  } else{\n    ans_dt[i] = 1\n  }\n}\n    # check result\nmean(ans_dt == train$Survived)\ntable(ans_dt)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# svm\nlibrary(e1071)\n\nfit_svm <- svm(factor(survived) ~ age + fare + sex + embarked + family \n                 + title + cabin + pclass,data = new_train)\n\n    # predicted result of regression\nsvm.fitted = predict(fit_svm)\nans_svm = rep(NA,891)\nfor(i in 1:891){\n  ans_svm[i] = as.integer(svm.fitted[[i]]) - 1\n}\n    # check result\nmean(ans_svm == train$Survived)\ntable(ans_svm)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 3.3 Model  Evaluation\n\nWe built 4 basic learner in last section. Then we are going to evaluate model accuracy using ```Confusion Matrix```.\n\n![enter image description here][1]\n\n\n  [1]: http://i64.tinypic.com/24lvbrk.png\n\n**Logistic Regression:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# logistic\na = sum(ans_logit ==1 & f.survived == 1)\nb = sum(ans_logit ==1 & f.survived == 0)\nc = sum(ans_logit ==0 & f.survived == 1)\nd = sum(ans_logit ==0 & f.survived == 0)\ndata.frame(a,b,c,d)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n![enter image description here][1]\n\n\n  [1]: http://i66.tinypic.com/b8tm69.png\n\n**Random Forest:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Random Forest\na = sum(ans_rf ==1 & f.survived == 1)\nb = sum(ans_rf ==1 & f.survived == 0)\nc = sum(ans_rf ==0 & f.survived == 1)\nd = sum(ans_rf ==0 & f.survived == 0)\ndata.frame(a,b,c,d)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "![enter image description here][1]\n\n\n  [1]: http://i67.tinypic.com/2saxr8k.png\n\n**Decision Tree:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Decision Tree\na = sum(ans_dt ==1 & f.survived == 1)\nb = sum(ans_dt ==1 & f.survived == 0)\nc = sum(ans_dt ==0 & f.survived == 1)\nd = sum(ans_dt ==0 & f.survived == 0)\ndata.frame(a,b,c,d)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "![enter image description here][1]\n\n\n  [1]: http://i68.tinypic.com/qravl3.png\n\n**SVM:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# SVM\na = sum(ans_svm ==1 & f.survived == 1)\nb = sum(ans_svm ==1 & f.survived == 0)\nc = sum(ans_svm ==0 & f.survived == 1)\nd = sum(ans_svm ==0 & f.survived == 0)\ndata.frame(a,b,c,d)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "![enter image description here][1]\n\n\n  [1]: http://i64.tinypic.com/2lsxx8o.png\n\nFrom matrix above, we can see that all models predict non-survival better than survival. And both logistic regression and SVM work well for training data set. Here, logistic regression has accuracy = 0.837, SVM has accuracy = 0.836.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n\n## 4. Prediction\n\nSince we got models that have reasonable predictive power, we can perform them to our test data set to make prediction. Here we choose SVM to perform prediction as an example.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# construct testing data frame\ntest_data_set <- data.frame(age = t.age, fare = t.fare, sex = t.sex, embarked = t.embarked, \n                            family = t.family, title = t.title,cabin =  t.cabin, pclass = t.pclass)\n# make prediction\nsvm_predict = predict(fit_svm,newdata = test_data_set )\nans_svm_predict = rep(NA,418)\nfor(i in 1:418){\n  ans_svm_predict[i] = as.integer(svm_predict[[i]]) - 1\n}\ntable(ans_svm_predict)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# create a csv file for submittion\nd<-data.frame(PassengerId = test$PassengerId, Survived = ans_svm_predict)\nwrite.csv(d,file = \"TitanicResult.csv\",row.names = F)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}